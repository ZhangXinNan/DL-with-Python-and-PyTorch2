{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2章 PyTorch基础\n",
    "## 2.2 安装配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support CUDA ?:  True\n",
      "tensor([10.], device='cuda:0')\n",
      "tensor([[ 0.0681,  0.4792, -0.8046],\n",
      "        [-1.5137,  0.9180,  1.3231]], device='cuda:0')\n",
      "tensor([[10.0681, 10.4792,  9.1954],\n",
      "        [ 8.4863, 10.9180, 11.3231]], device='cuda:0')\n",
      "Support cudnn ?:  True\n"
     ]
    }
   ],
   "source": [
    "#运行文件test_gpy.py,查看GPU的配置信息\n",
    "!python test_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 NumPy与Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1  torch概述\n",
    "对Tensor的操作很多，从接口的角度来划分，可以分为两类：  \n",
    "1）torch.function，如torch.sum、torch.add等，  \n",
    "2）tensor.function，如tensor.view、tensor.add等。  \n",
    "\t这些操作对大部分Tensor都是等价的，如torch.add(x,y)与x.add(y)等价。在实际使用时，可以根据个人爱好选择。\n",
    "\t如果从修改方式的角度，可以分为以下两类。  \n",
    "1）不修改自身数据，如x.add(y),x的数据不变，返回一个新的tensor。  \n",
    "2）修改自身数据，如x.add_(y)（运行符带下划线后缀），运算结果存在x中，x被修改。  \n",
    "以下代码说明add与add_的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6])\n",
      "tensor([1, 2])\n",
      "tensor([4, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "z=x.add(y)\n",
    "print(z)\n",
    "print(x)\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 创建tensor\n",
    "新建Tensor的方法很多，可以把列表或ndarray等数据对象直接转换为Tensor，也可以根据指定的形状构建。常见的构建Tensor的方法，可参考表2-1。\n",
    "\n",
    "|表2-1 常见的新建Tensor方法|\n",
    "|--------------------------|\n",
    "\n",
    "|函数\t|功能|\n",
    "|:-------------|:---------------------------------------------------|\n",
    "Tensor(*size)\t|直接从参数构造一个的张量，支持list、numpy数组|\n",
    "eye(row, column)\t|创建指定行数，列数的二维单位tensor|\n",
    "linspace(start,end,steps)\t|从step到end，均匀切分成steps份|\n",
    "logspace(start,end,steps)\t|从10^step, 到10^end，均匀切分成steps份|\n",
    "rand/randn(*size)\t|生成[0,1)均匀分布/标准正态分布数据|\n",
    "ones(*size)\t|返回指定shape的张量，元素初始为1|\n",
    "zeros(*size)\t|返回指定shape的张量，元素初始为0|\n",
    "ones_like(t)\t|返回与t的shape相同的张量，且元素初始为1|\n",
    "zeros_like(t)\t|返回与t的shape相同的张量，且元素初始为0|\n",
    "arange(start,end,step)\t|在区间[start,end)上以间隔step生成一个序列张量|\n",
    "from_numpy(ndarray) \t|从ndarray创建一个tensor|\n",
    "\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#根据list数据生成tensor\n",
    "torch.Tensor([1,2,3,4,5,6])\n",
    "#根据指定形状生成tensor\n",
    "torch.Tensor(2,3)\n",
    "#根据给定的tensor的形状\n",
    "t=torch.Tensor([[1,2,3],[4,5,6]])\n",
    "#查看tensor的形状\n",
    "t.size()\n",
    "#shape与size()等价方式\n",
    "t.shape\n",
    "#根据已有形状创建tensor\n",
    "torch.Tensor(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【说明】注意torch.Tensor与torch.tensor的几点区别  \n",
    "1）torch.Tensor是torch.empty和torch.tensor之间的一种混合，但是，当传入数据时，torch.Tensor使用全局默认dtype（FloatTensor），torch.tensor从数据中推断数据类型。  \n",
    "2）torch.tensor(1)返回一个固定值1，而torch.Tensor(1)返回一个大小为1的张量，它是随机初始化的值。  \n",
    "举例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1的值tensor([0.]),t1的数据类型torch.FloatTensor\n",
      "t2的值1,t2的数据类型torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "#torch.tensor与torch.Tentor的区别\n",
    "import torch\n",
    "t1=torch.Tensor(1)\n",
    "t2=torch.tensor(1)\n",
    "print(\"t1的值{},t1的数据类型{}\".format(t1,t1.type()))\n",
    "print(\"t2的值{},t2的数据类型{}\".format(t2,t2.type()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t1的值tensor([3.5731e-20]),t1的数据类型torch.FloatTensor  \n",
    "t2的值1,t2的数据类型torch.LongTensor  \n",
    "下面来看一些根据一定规则，自动生成tensor的例子。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#生成一个单位矩阵\n",
    "torch.eye(2,2)\n",
    "#自动生成全是0的矩阵\n",
    "torch.zeros(2,3)\n",
    "#根据规则生成数据\n",
    "torch.linspace(1,10,4)\n",
    "#生成满足均匀分布随机数\n",
    "torch.rand(2,3)\n",
    "#生成满足标准分布随机数\n",
    "torch.randn(2,3)\n",
    "#返回所给数据形状相同，值全为0的张量\n",
    "torch.zeros_like(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 修改tensor的形状\n",
    "在处理数据、构建网络层等过程中，我们经常需要了解Tensor的形状、改变Tensor的形状。与改变NumPy的形状类似，改变tenor的形状也有很多类似函数，具体可参考表2-2。  \t                     \n",
    "\n",
    "|表2-2 为tensor常用修改形状的函数。|\n",
    "|----------------------------------|\n",
    "\n",
    "|函数\t|说明|\n",
    "|:--------|:---------------------------------------------------|\n",
    "|size()\t|返回张量的shape属性值,与函数shape(0.4版新增)等价|\n",
    "|numel(input)\t|计算tensor的元素个数|\n",
    "|view(*shape)\t| 修改tensor的shape，与reshape(0.4版新增)类似，但view返回的对象与源tensor共享内存，修改一个另一个同时修改。Reshape将生成新的tensor，而且不要求源tensor是连续的。View(-1)展平数组。|\n",
    "|resize\t|类似于view，但在size超出时会重新分配内存空间|\n",
    "|item\t|若tensor为单元素，则返回pyton的标量|\n",
    "|unsqueeze\t|在指定维度增加一个\"1\"|\n",
    "|squeeze\t|在指定维度压缩一个\"1\"|\n",
    "\n",
    "下面来看一些实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#生成一个形状为2x3的矩阵\n",
    "x = torch.randn(2, 3)\n",
    "#查看矩阵的形状\n",
    "x.size()   #结果为torch.Size([2, 3])\n",
    "\n",
    "#查看x的维度\n",
    "x.dim()    #结果为2\n",
    "#把x变为3x2的矩阵\n",
    "x.view(3,2)\n",
    "#把x展平为1维向量\n",
    "y=x.view(-1)  \n",
    "y.shape\n",
    "#添加一个维度\n",
    "z=torch.unsqueeze(y,0)\n",
    "#查看z的形状\n",
    "z.size()   #结果为torch.Size([1, 6])\n",
    "#计算Z的元素个数\n",
    "z.numel()   #结果为6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 索引操作\n",
    "\tTensor的索引操作与NumPy类似，一般情况下索引结果与源数据共享内存。从tensor获取元素除了可以通过索引，也可借助一些函数，常用的选择函数可参考表2-3。\n",
    "|表2-3 常用选择操作函数|\n",
    "|----------------------|\n",
    "\n",
    "|函数\t|说明|\n",
    "|:---------------------------|:-------------------------------------|\n",
    "index_select(input,dim,index)\t|在指定维度上选择一些行或列|\n",
    "nonzero(input)\t|获取非0元素的下标|\n",
    "masked_select(input,mask)\t|使用二元值进行选择|\n",
    "gather(input,dim,index)\t|在指定维度上选择数据,输出的形状与index（index的类型必须是LongTensor类型的）一致|\n",
    "scatter_( input, dim, index, src)\t|为gather的反操作，根据指定索引补充数据|\n",
    "\n",
    "以下为部分函数的实现代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3607, -0.2859,  0.0000],\n",
       "        [ 0.0000, -1.3833,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#设置一个随机种子\n",
    "torch.manual_seed(100) \n",
    "#生成一个形状为2x3的矩阵\n",
    "x = torch.randn(2, 3)\n",
    "#根据索引获取第1行，所有数据\n",
    "x[0,:]\n",
    "#获取最后一列数据\n",
    "x[:,-1]\n",
    "#生成是否大于0的Byter张量\n",
    "mask=x>0\n",
    "#获取大于0的值\n",
    "torch.masked_select(x,mask)\n",
    "#获取非0下标,即行，列索引\n",
    "torch.nonzero(mask)\n",
    "#获取指定索引对应的值,输出根据以下规则得到\n",
    "#out[i][j] = input[index[i][j]][j]  # if dim == 0\n",
    "#out[i][j] = input[i][index[i][j]]  # if dim == 1\n",
    "index=torch.LongTensor([[0,1,1]])\n",
    "torch.gather(x,0,index)\n",
    "index=torch.LongTensor([[0,1,1],[1,1,1]])\n",
    "a=torch.gather(x,1,index)\n",
    "#把a的值返回到一个2x3的0矩阵中\n",
    "z=torch.zeros(2,3)\n",
    "z.scatter_(1,index,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 广播机制\n",
    "前文1.8节介绍了NumPy的广播机制，它是向量运算的重要技巧。PyTorch也支持广播规则，下面通过几个示例进行说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [10, 11, 12],\n",
      "        [20, 21, 22],\n",
      "        [30, 31, 32]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "A = np.arange(0, 40,10).reshape(4, 1)\n",
    "B = np.arange(0, 3)\n",
    "#把ndarray转换为Tensor\n",
    "A1=torch.from_numpy(A)  #形状为4x1\n",
    "B1=torch.from_numpy(B)  #形状为3\n",
    "#Tensor自动实现广播\n",
    "C=A1+B1\n",
    "#我们可以根据广播机制，手工进行配置\n",
    "#根据规则1，B1需要向A1看齐，把B变为（1,3）\n",
    "B2=B1.unsqueeze(0)  #B2的形状为1x3\n",
    "#使用expand函数重复数组，分别的4x3的矩阵\n",
    "A2=A1.expand(4,3)\n",
    "B3=B2.expand(4,3)\n",
    "#然后进行相加,C1与C结果一致\n",
    "C1=A2+B3\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 遂元素操作\n",
    "与NumPy一样，tensor也有逐元素操作，操作内容相似，但使用函数可能不尽相同。大部分数学运算都属于逐元操作，逐元素操作输入与输出的形状相同。，常见的逐元素操作，可参考表2-4。\n",
    "\n",
    "|表2-4常见逐元素操作|\n",
    "|-------------------|\n",
    "                    \n",
    "|函数\t|说明|\n",
    "|:-------------------|:-------------------------------|\n",
    "abs/add\t|绝对值/加法|\n",
    "addcdiv(t,t1,t2,value=1)\t|t1与t2的按元素除后，乘value加t|\n",
    "addcmul(t,t1,t2, value=1)\t|t1与t2的按元素乘后，乘value加t|\n",
    "ceil/floor\t|向上取整/向下取整|\n",
    "clamp(t, min, max)\t|将张量元素限制在指定区间|\n",
    "exp/log/pow\t|指数/对数/幂|\n",
    "mul(或*)/neg\t|逐元素乘法/取反|\n",
    "sigmoid/tanh/softmax\t|激活函数|\n",
    "sign/sqrt\t|取符号/开根号|\n",
    "\n",
    "【说明】这些操作均创建新的tensor，如果需要就地操作，可以使用这些方法的下划线版本，例如abs_。  \n",
    "以下为部分逐元素操作代码实例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6828, 1.1340, 3.7482]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(1, 3)\n",
    "t1 = torch.randn(3, 1)\n",
    "t2 = torch.randn(1, 3)\n",
    "#t+0.1*(t1/t2)\n",
    "torch.addcdiv(t, t1, t2,value=0.1)\n",
    "#计算sigmoid\n",
    "torch.sigmoid(t)\n",
    "#将t限制在[0,1]之间\n",
    "torch.clamp(t,0,1)\n",
    "#t+2进行就地运算\n",
    "t.add_(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.7 归并操作\n",
    "归并操作，顾名思义，就是对输入进行归并或合计等操作，这类操作的输入输出形状一般不相同，而且往往是输入大于输出形状。归并操作可以对整个tensor进行归并，也可以沿着某个维度进行归并。常见的归并操作可参考表2-5。\n",
    "\n",
    "|表2-5 常见的归并操作|\n",
    "|--------------------|\n",
    "\n",
    "|函数\t|说明|\n",
    "|:--------------|:----------------------|\n",
    "cumprod(t, axis)\t|在指定维度对t进行累积|\n",
    "cumsum\t|在指定维度对t进行累加|\n",
    "dist(a,b,p=2)\t|返回a,b之间的p阶范数|\n",
    "mean/median\t|均值/中位数|\n",
    "std/var\t|标准差/方差|\n",
    "norm(t,p=2)\t|返回t的p阶范数|\n",
    "prod(t)/sum(t)\t|返回t所有元素的积/和|\n",
    "\n",
    "【说明】\n",
    "\t归并操作一般涉及一个dim参数，指定沿哪个维进行归并。另一个参数是keepdim，说明输出结果中是否保留维度1，默认情况是False，即不保留。  \n",
    "以下为归并操作的部分代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#生成一个含6个数的向量\n",
    "a=torch.linspace(0,10,6)\n",
    "#使用view方法，把a变为2x3矩阵\n",
    "a=a.view((2,3))\n",
    "#沿y轴方向累加，即dim=0\n",
    "b=a.sum(dim=0)   #b的形状为[3]\n",
    "#沿y轴方向累加，即dim=0,并保留含1的维度\n",
    "b=a.sum(dim=0,keepdim=True) #b的形状为[1,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.8比较操作\n",
    "比较操作一般进行逐元素比较，有些是按指定方向比较。常用的比较函数可参考表2-6。\n",
    "\n",
    "|表2-6 常用的比较函数|\n",
    "|--------------------|\n",
    "\n",
    "|函数\t|说明|\n",
    "|:--------|:-------------------------|\n",
    "eq\t|比较tensor是否相等，支持broadcast|\n",
    "equal\t|比较tensor是否有相同的shape与值|\n",
    "ge/le/gt/lt\t|大于/小于比较/大于等于/小于等于比较|\n",
    "max/min(t,axis)\t|返回最值，若指定axis，则额外返回下标|\n",
    "topk(t,k,axis)\t|在指定的axis维上取最高的K个值 | \n",
    "\n",
    "以下是部分函数的代码实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[ 6.,  8., 10.]]),\n",
       "indices=tensor([[1, 1, 1]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.linspace(0,10,6).view(2,3)\n",
    "#求所有元素的最大值\n",
    "torch.max(x)   #结果为10\n",
    "#求y轴方向的最大值\n",
    "torch.max(x,dim=0)  #结果为[6,8,10]\n",
    "#求最大的2个元素\n",
    "torch.topk(x,1,dim=0)  #结果为[6,8,10],对应索引为tensor([[1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.9 矩阵操作\n",
    "机器学习和深度学习中存在大量的矩阵运算，用的比较多的有两种，一种是逐元素乘法，另外一种是点积乘法。PyTorch中常用的矩阵函数可参考表2-7。\n",
    "\n",
    "|表2-7 常用矩阵函数|\n",
    "|------------------|\n",
    "\n",
    "|函数\t|说明|\n",
    "|:---------|:---------------------------------|\n",
    "dot(t1, t2)\t|计算张量(1D)的内积或点积|\n",
    "mm(mat1, mat2)/bmm(batch1,batch2)\t|计算矩阵乘法/含batch的3D矩阵乘法|\n",
    "mv(t1, v1)\t|计算矩阵与向量乘法|\n",
    "t\t|转置|\n",
    "svd(t)\t|计算t的SVD分解|\n",
    "\n",
    "【说明】  \n",
    "1）torch的dot与NumPy的dot有点不同，torch中dot对两个为1维张量进行点积运算，NumPy中的dot无此限制。  \n",
    "2）mm是对2维矩阵进行点积运算，bmm对含batch的3维矩阵进行点积运算。  \n",
    "3）转置运算会导致存储空间不连续，需要调用contiguous方法转为连续。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28, 77, 30, 27],\n",
       "         [24, 73, 25, 21]],\n",
       "\n",
       "        [[15, 68, 57,  6],\n",
       "         [25, 39, 43, 10]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.tensor([2, 3])\n",
    "b=torch.tensor([3, 4])\n",
    "\n",
    "torch.dot(a,b)  #运行结果为18\n",
    "x=torch.randint(10,(2,3))\n",
    "y=torch.randint(6,(3,4))\n",
    "torch.mm(x,y)\n",
    "x=torch.randint(10,(2,2,3))\n",
    "y=torch.randint(6,(2,3,4))\n",
    "torch.bmm(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.10 PyTorch与NumPy比较\n",
    "\tPyTorch与NumPy有很多类似的地方，并且有很多相同的操作函数名称，或虽然函数名称不同但含义相同；当然也有一些虽然函数名称相同，但含义不尽相同。对此，有时很容易混淆，下面我们把一些主要的区别进行汇总，具体可参考表2-8。\n",
    "\n",
    "|表2-8  PyTorch与NumPy函数对照表|\n",
    "|-------------------------------|\n",
    "\n",
    "|操作类别\t|NumPy\t|PyTorch|\n",
    "|:----------|:--------|:-----------------------|\n",
    "|数据类型\t|np.ndarray\t|torch.Tensor|\n",
    "|\t|np.float32\t|torch.float32; torch.float|\n",
    "|\t|np.float64\t|torch.float64; torch.double|\n",
    "|\t|np.int64\t|torch.int64; torch.long|\n",
    "|从已有数据构建\t|np.array([3.2, 4.3], dtype=np.float16)\t|torch.tensor([3.2, 4.3],dtype=torch.float16)|\n",
    "|\t|x.copy()|\tx.clone()|\n",
    "|\t|np.concatenate\t|torch.cat|\n",
    "|线性代数|\tnp.dot\t|torch.mm|\n",
    "|属性|\tx.ndim\t|x.dim()|\n",
    "|\t|x.size\t|x.nelement()|\n",
    "|形状操作\t|x.reshape\t|x.reshape; x.view|\n",
    "|\t|x.flatten\t|x.view(-1)|\n",
    "|类型转换\t|np.floor(x)\t|torch.floor(x); x.floor()|\n",
    "|比较|np.less\t|x.lt|\n",
    "|\t|np.less_equal/np.greater\t|x.le/x.gt|\n",
    "|\t|np.greater_equal/np.equal/np.not_equal|\tx.ge/x.eq/x.ne|\n",
    "|随机种子\t|np.random.seed\t|torch.manual_seed|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
